{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "twelve-reporter",
   "metadata": {},
   "source": [
    "Adding all datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secondary-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-circus",
   "metadata": {},
   "source": [
    "### Conan dataset:\n",
    "\n",
    "Hate  speech  collection: For  each  language we asked two native speaker experts (NGO train-ers) to write around 50 prototypical islamophobic short hate texts. This step was used to ensure that:(i) the sample uniformly covers the typical ‘arguments’ against Islam as much as possible, (ii) wecan distribute to the NLP community the originalhate speech as well as its counter-narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "empty-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'..\\data\\conan\\CONAN.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "conan = (pd.DataFrame.from_dict(data=data['conan'][0], orient='index')).T\n",
    "for i in range(len(data['conan'])):\n",
    "    sett = (pd.DataFrame.from_dict(data=data['conan'][i], orient='index')).T\n",
    "    if sett['cn_id'][0][0:2] == 'EN':\n",
    "        conan = conan.append(sett, ignore_index=True)\n",
    "conan_dt = pd.DataFrame(conan.hateSpeech.unique())\n",
    "conan_dt['classification'] = 'islamophobic'\n",
    "conan_dt.columns = ['text', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-message",
   "metadata": {},
   "source": [
    "### LoL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lonely-vacuum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zigga\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\generic.py:5170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "lol = pd.read_csv (r'..\\data\\lol\\lol_data_ok.csv')\n",
    "lol_data = lol[lol['classification'] != 'neutral']\n",
    "lol_data.classification = lol_data.classification.replace('harrasment', 'cyberbullying')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-neighbor",
   "metadata": {},
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fiscal-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv (r'..\\data\\reddit_binary-jing-qian\\reddit.csv')\n",
    "red_data = reddit[reddit['hate_speech_idx'].isnull() == False][['text']]\n",
    "red_data['classification'] = 'hateful'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-spending",
   "metadata": {},
   "source": [
    "### Gab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civilian-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "gab = pd.read_csv(r'..\\data\\gab_binary-jing-qian\\gab.csv')\n",
    "gab_data = gab[~gab['hate_speech_idx'].isnull()][['text']]\n",
    "gab_data['classification'] = 'hateful'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-advertising",
   "metadata": {},
   "source": [
    "### Twitter hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governmental-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_hierarchy = pd.read_csv (r'..\\data\\twitter_hierarchy_t-davidson\\labeled_data.csv')\n",
    "twitter_h = twitter_hierarchy[twitter_hierarchy['offensive_language'] > 0][['tweet']]\n",
    "twitter_h['classification'] = 'offensive' \n",
    "twitter_h.columns = ['text', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-compound",
   "metadata": {},
   "source": [
    "### Abusive speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unsigned-sudan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hate_speech = pd.read_csv (r'..\\data\\multilingual_and_multi-aspect_hate_speech_analysis\\hate_speech_mlma\\en_dataset.csv')\n",
    "abusive = hate_speech[hate_speech['sentiment'].str.contains(\"abusive\")][['tweet']]\n",
    "abusive['classification'] = 'abusive' \n",
    "abusive.columns = ['text', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-globe",
   "metadata": {},
   "source": [
    "### Online hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "declared-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_attack = pd.read_csv(r'..\\data\\personal_attacks_seen_at_scale_wulczyn\\train.csv')\n",
    "insults = personal_attack[personal_attack.Insult == 1][['Comment']]\n",
    "insults['classification'] = 'insult' \n",
    "insults.columns = ['text', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-differential",
   "metadata": {},
   "source": [
    "### White suppremacy forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "economic-french",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(r'..\\data\\stormfront_ternary_vicomtech\\annotations_metadata.csv')\n",
    "hates = list()\n",
    "for file in annotations[annotations['label'] == 'hate'].file_id.unique():\n",
    "    hates_dt = pd.read_csv('../data/stormfront_ternary_vicomtech/all_files/' + str(file) + '.txt', header = None)\n",
    "    hates.append(hates_dt[0][0])\n",
    "hates = pd.DataFrame(hates)\n",
    "hates['classification'] = 'hateful' \n",
    "hates.columns = ['text', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-softball",
   "metadata": {},
   "source": [
    "### Multimodal twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "statistical-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter2 = pd.read_excel('twitter_mutlimodal_hate_speech.xlsx', sheet_name=None)\n",
    "twitter2 = twitter2['Sheet1'][['text', 'Homophobe', 'NotHate', 'OtherHate', 'Racist', 'Religion', 'Sexist']]\n",
    "twitter_melted = pd.melt(twitter2, id_vars=['text'], value_vars=['Homophobe', 'NotHate', 'OtherHate', 'Racist', 'Religion', 'Sexist'])\n",
    "twitter_melted = twitter_melted[twitter_melted['value'] == True][['text', 'variable']]\n",
    "twitter_melted.columns = ['text', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-asian",
   "metadata": {},
   "source": [
    "### Appending datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "functioning-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = conan_dt\n",
    "dataset = dataset.append(conan_dt, ignore_index=True)\n",
    "dataset = dataset.append(lol_data, ignore_index=True)\n",
    "dataset = dataset.append(red_data, ignore_index=True)\n",
    "dataset = dataset.append(twitter_h, ignore_index=True)\n",
    "dataset = dataset.append(abusive, ignore_index=True)\n",
    "dataset = dataset.append(insults, ignore_index=True)\n",
    "dataset = dataset.append(hates, ignore_index=True)\n",
    "dataset = dataset.append(gab_data, ignore_index=True)\n",
    "dataset = dataset.append(twitter_melted, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "progressive-pioneer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['islamophobic', 'cyberbullying', 'hateful', 'offensive', 'abusive',\n",
       "       'insult', 'Homophobe', 'NotHate', 'OtherHate', 'Racist',\n",
       "       'Religion', 'Sexist'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classification.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stupid-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('..\\data\\main_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
